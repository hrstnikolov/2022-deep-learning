{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5916d5af-0811-4865-a229-37de3ff37bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8d248a1c-4bb2-414c-84e9-14b30ee3ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from IPython.display import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import text_dataset_from_directory\n",
    "from tensorflow.keras.layers import TextVectorization, Input\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50555180-45ef-4710-9ca8-a28c989e8356",
   "metadata": {},
   "source": [
    "# Basic Text Classification (repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a7730-cd42-42ac-9cd0-aabd2f38deab",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5146bf-e6c6-4744-8f10-566389da2595",
   "metadata": {},
   "source": [
    "### Define constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "00922df7-5be0-40a4-8a06-09a3383e56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "BATCH_SIZE = 32\n",
    "MAX_TOKENS = 10_000  # the vocabulary would contain max of 10K words+ngrams\n",
    "OUTPUT_SEQUENCE_LENGTH = 250  # each text(record) shall be limited to 250 words(+ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63a8f2-3362-43ff-909d-fe680824a472",
   "metadata": {},
   "source": [
    "### List the contents of the dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1455ae-be07-4309-8e52-2b87f43d6704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_path = os.path.join(\n",
    "    os.environ['HOME'],\n",
    "    'Desktop/datasets/aclImdb',\n",
    ")\n",
    "\n",
    "os.listdir(movies_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6039d0-c7e7-44b2-833e-56343df7d824",
   "metadata": {},
   "source": [
    "### Print out a sample of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab70678b-73a1-4e23-830c-bea44065e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the, and, a, of, to, is, it, in, i, this\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(movies_path, 'imdb.vocab'), encoding='ISO-8859-1') as f:\n",
    "    vocab = [v.strip() for v in f.readlines()[:10]]\n",
    "    print(', '.join(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e237ca-bd12-4f48-b22d-9635d16396c6",
   "metadata": {},
   "source": [
    "### Print out a sample review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960b1d6e-ced0-4e29-909d-489f3eb1aee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any story comprises a premise, characters and conflict. Characters plotting their own play promises triumph, and a militant character readily lends oneself to this. Ardh Satya's premise is summarized by the poem of the same name scripted by Dilip Chitre. The line goes - \"ek palde mein napunsaktha, doosre palde mein paurush, aur teek tarazu ke kaante par, ardh satya ?\". A rough translation - \"The delicate balance of right & wrong ( commonly seen on the busts of blind justice in the courts ) has powerlessness on one plate and prowess on another. Is the needle on the center a half-truth ? \"<br /><br />The poem is recited midway in the film by Smita Patil to Om Puri at a resturant. It makes a deep impact on the protagonist & lays the foundation for much of the later events that follow. At the end of the film, Om Puri ends up in exactly the same situation described so aptly in the poem.<br /><br />The film tries mighty hard to do a one-up on the poem. However, Chitre's words are too powerful, and at best, the film matches up to the poem in every aspect.<br /><br />\n"
     ]
    }
   ],
   "source": [
    "path_train_positive = os.path.join(movies_path, 'train', 'pos')\n",
    "a_file = os.listdir(path_train_positive)[123]\n",
    "with open(os.path.join(path_train_positive, a_file)) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c205926-38e5-4510-abed-efe4c9630de4",
   "metadata": {},
   "source": [
    "### Define train and test directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f5c2da0-9c34-4ab0-bbd0-518de43b8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = os.path.join(movies_path, 'train')\n",
    "path_test = os.path.join(movies_path, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c02ee1-10e3-4af4-b805-40016c871fcc",
   "metadata": {},
   "source": [
    "### Read raw training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace9f74e-b773-45c3-94f3-203b681549c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Using 5000 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
       " <BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_ds, raw_val_ds = text_dataset_from_directory(\n",
    "    directory=path_train,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=['pos', 'neg'],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=None,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    validation_split=0.2,\n",
    "    subset='both',\n",
    ")\n",
    "\n",
    "raw_training_ds, raw_val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f03588-c6b9-46e2-bbb4-bca45d54215b",
   "metadata": {},
   "source": [
    "### Verify the number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f46b90e2-4e63-4918-9d21-f7b9d1936c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5024)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_training = raw_training_ds.cardinality().numpy() * BATCH_SIZE\n",
    "n_validation = raw_val_ds.cardinality().numpy() * BATCH_SIZE\n",
    "\n",
    "n_training, n_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1bc36-043b-4b77-b0cb-a15e70459560",
   "metadata": {},
   "source": [
    "### Print out a sample review and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "553c5fa7-3706-45b2-af2b-1eb5290e7361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'After, I watched the films... I thought, \"Why the heck was this film such a high success in the Korean Box Office?\" Even thought the movie had a clever/unusal scenario, the acting wasn\\'t that good and the characters weren\\'t very interesting. For a Korean movie... I liked the fighting scenes. If you want to watch a film without thinking, this is the film for you. But I got to admit... the film was kind of childish... 6/10'\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for texts_batch, labels_batch in raw_training_ds.take(1):\n",
    "    sample_review = texts_batch[0].numpy()\n",
    "    sample_label = labels_batch[0].numpy()\n",
    "    print(sample_review, sample_label, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27fb19-2f7c-4d85-9da0-9e042369c9fd",
   "metadata": {},
   "source": [
    "## Prepare the dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a7817-6dd2-43d8-9df4-d7f95100252f",
   "metadata": {},
   "source": [
    "### Implement preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccd7bba3-9cec-47cd-b93f-f9079145dba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'After, I watched the films... I thought, \"Why the heck was this film such a high success in the Korean Box Office?\" Even thought the movie had a clever/unusal scenario, the acting wasn\\'t that good and the characters weren\\'t very interesting. For a Korean movie... I liked the fighting scenes. If you want to watch a film without thinking, this is the film for you. But I got to admit... the film was kind of childish... 6/10'\n",
      "\n",
      "tf.Tensor(b'after i watched the films i thought why the heck was this film such a high success in the korean box office even thought the movie had a cleverunusal scenario the acting wasnt that good and the characters werent very interesting for a korean movie i liked the fighting scenes if you want to watch a film without thinking this is the film for you but i got to admit the film was kind of childish 610', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Implement a function to clean each review.\n",
    "def clean_review(review):\n",
    "    cleaned = tf.strings.lower(review)\n",
    "    cleaned = tf.strings.regex_replace(cleaned, '<[^>]+>', '')\n",
    "    cleaned = tf.strings.regex_replace(cleaned, f'[{re.escape(string.punctuation)}]', '')\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "print(sample_review, clean_review(sample_review), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717428bb-0390-41aa-97b0-9e573cd3552f",
   "metadata": {},
   "source": [
    "### Define layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e39fcedc-7b92-4274-a2df-9e0297ec1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    standardize=clean_review,\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=OUTPUT_SEQUENCE_LENGTH,\n",
    "    encoding='utf-8',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a6de9-51f7-461f-989c-a7e59df2eda8",
   "metadata": {},
   "source": [
    "### Extract just the reviews (drop labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acd53f9e-c113-45a8-ba8a-d7b4758cc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds = raw_training_ds.map(lambda review, label: review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8992b927-210d-438a-9271-97b22b05eda5",
   "metadata": {},
   "source": [
    "### Compute the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "496f5bfa-7112-4f8e-bf53-97b206f22be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocab is ordered (most frequent words first).\n",
    "vectorize_layer.adapt(training_ds)\n",
    "vectorize_layer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da8743b-825a-4d32-b909-2f2361f0916f",
   "metadata": {},
   "source": [
    "### Vectorize a sample review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3b9b489e-1344-4911-9f3f-72789fdc2f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'This is a wonderful movie in a lot of ways. Everyone in my family enjoyed it. The animation is excellent and easily demonstrates that there are plenty of producers who create films that are as visually brilliant as anything that comes from the Disney Studio.<br /><br />One difference from the normal Disney fare is that this Dreamworks movie does not feature some wise-cracking side kick for comedy relief. And, there are no sudden moments where the characters break into song. I am sure that a scene at the beginning of the film would not appear in a Disney picture: the birth of Spirit. But it is done tastefully and is not offensive at all. \"Spirit\" was a great breath of fresh air. Don\\'t get me wrong. I have loved Disney for years and will continue to do so. <br /><br />\"Spirit\" is another example of great animated fare. As soon as it was over, my kids wanted to watch it again. I had the same feeling. I thoroughly recommend it.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(250,), dtype=int64, numpy=\n",
       "array([  10,    7,    4,  368,   17,    8,    4,  168,    5,  745,  302,\n",
       "          8,   58,  224,  493,    9,    2,  730,    7,  311,    3,  678,\n",
       "       5277,   12,   48,   23,  939,    5, 1156,   35,  966,   96,   12,\n",
       "         23,   14, 1965,  522,   14,  226,   12,  250,   36,    2,  900,\n",
       "          1, 1383,   36,    2, 1196,  900, 2470,    7,   12,   10,    1,\n",
       "         17,  120,   21,  788,   46, 9125,  505, 2164,   16,  220, 2137,\n",
       "          3,   48,   23,   56, 1974,  388,  112,    2,  101,  987,   78,\n",
       "        609,   11,  236,  240,   12,    4,  131,   30,    2,  443,    5,\n",
       "          2,   19,   57,   21,  925,    8,    4,  900,  430,    2, 2435,\n",
       "          5, 1071,   18,    9,    7,  219,    1,    3,    7,   21, 2320,\n",
       "         30,   31, 1071,   13,    4,   82, 2939,    5, 1488, 1003,   89,\n",
       "         76,   69,  355,   11,   25,  425,  900,   16,  147,    3,   74,\n",
       "       1719,    6,   81,   38, 1071,    7,  154,  441,    5,   82, 1084,\n",
       "       2470,   14,  506,   14,    9,   13,  123,   58,  322,  455,    6,\n",
       "        103,    9,  171,   11,   67,    2,  163,  520,   11, 1504,  369,\n",
       "          9,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int64)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The review printed below is before preprocessing \n",
    "# that is before lowercasing, removing punctuation and html.\n",
    "sample_review = list(training_ds.as_numpy_iterator())[0][0]\n",
    "display(sample_review)\n",
    "display(vectorize_layer(sample_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d854a3-7126-44a4-a03f-8357c9f249ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73898198-e3f6-4a8f-9c9f-cf879152b0de",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## DEMO `TextVectorization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c129cc0-4a99-4986-956a-8a4b92412f7e",
   "metadata": {},
   "source": [
    "### Define a Vectorizer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9cd70d14-1b08-4094-b196-8697517ba84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(name='dummy_vectorizer')\n",
    "texts = [\n",
    "\t\"She she doesn’t study German on Monday.\",\n",
    "\t\"Does she live in Paris?\",\n",
    "\t\"He doesn’t teach math.\",\n",
    "\t\"Cats hate water.\",\n",
    "\t# \"Every child likes an ice cream.\",\n",
    "\t# \"My brother takes out the trash.\",\n",
    "\t# \"The course starts next Sunday.\",\n",
    "\t# \"She swims every morning.\",\n",
    "\t# \"I don’t wash the dishes.\",\n",
    "\t# \"We see them every week.\",\n",
    "\t# \"I don’t like tea.\",\n",
    "\t# \"When does the train usually leave?\",\n",
    "\t# \"She always forgets her purse.\",\n",
    "\t# \"You don’t have children.\",\n",
    "\t# \"I and my sister don’t see each other anymore.\",\n",
    "\t# \"They don’t go to school tomorrow.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580f5fc-6a04-4b7d-bf80-37e0daf25da1",
   "metadata": {},
   "source": [
    "### Teach vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ff1cfbc7-6a62-43a9-9dd0-397676a4aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15918a7-9a82-4bab-a00f-b4597446ed16",
   "metadata": {},
   "source": [
    "### Display some attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "382a1849-9e0a-4f2c-bde5-49ae2fa93172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'string'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dummy_vectorizer'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'she', 'doesn’t', 'water', 'teach', 'study', 'paris']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# `[UNK]` =  unknown word\n",
    "display(vectorizer.vocabulary_size())\n",
    "display(vectorizer.dtype)\n",
    "display(vectorizer.name)\n",
    "display(vectorizer.get_vocabulary()[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e921c0a-ced4-4e34-bb23-4e43e5c36ce8",
   "metadata": {},
   "source": [
    "### Default integer encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "74a8f483-4cc6-4740-9167-21102c0bdafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 7), dtype=int64, numpy=\n",
       "array([[ 2,  2,  3,  6, 15,  8,  9],\n",
       "       [16,  2, 11, 12,  7,  0,  0],\n",
       "       [13,  3,  5, 10,  0,  0,  0],\n",
       "       [17, 14,  4,  0,  0,  0,  0]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Default integer encoding: integer indices, one integer index per split string token.\n",
    "# Each sentence need to be a separate record -> `tf.expand_dims()`.\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f2742-0c25-42e1-af43-e7fe13c77364",
   "metadata": {},
   "source": [
    "### Cap the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aab59724-bac3-4090-afd4-3f440603340a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'she', 'doesn’t', 'water', 'teach', 'study', 'paris']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 7), dtype=int64, numpy=\n",
       "array([[2, 2, 3, 6, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 7, 0, 0],\n",
       "       [1, 3, 5, 1, 0, 0, 0],\n",
       "       [1, 1, 4, 0, 0, 0, 0]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cap the number of tokens -> a lot of `[UNK]` (unknown words).\n",
    "vectorizer = TextVectorization(max_tokens=8)\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9024a90-7041-470e-939d-b64fcb9e7adf",
   "metadata": {},
   "source": [
    "### Increase the len of the output encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "308c9ec0-aacf-406c-8c74-1a682cbf0957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'she', 'doesn’t', 'water', 'teach', 'study', 'paris']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 20), dtype=int64, numpy=\n",
       "array([[2, 2, 3, 6, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 2, 1, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Increase the len of the output encoding -> trailing zeros.\n",
    "vectorizer = TextVectorization(max_tokens=8, output_sequence_length=20)\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e4548-02db-442b-867d-606905c0d535",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b5f578d1-1198-487d-b520-152da6d9b269",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'she',\n",
       " 'doesn’t',\n",
       " 'water',\n",
       " 'teach math',\n",
       " 'teach',\n",
       " 'study german',\n",
       " 'study',\n",
       " 'she she',\n",
       " 'she live',\n",
       " 'she doesn’t',\n",
       " 'paris',\n",
       " 'on monday',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'math',\n",
       " 'live in',\n",
       " 'live',\n",
       " 'in paris',\n",
       " 'in',\n",
       " 'he doesn’t',\n",
       " 'he',\n",
       " 'hate water',\n",
       " 'hate',\n",
       " 'german on',\n",
       " 'german',\n",
       " 'doesn’t teach',\n",
       " 'doesn’t study',\n",
       " 'does she',\n",
       " 'does',\n",
       " 'cats hate',\n",
       " 'cats']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 13), dtype=int64, numpy=\n",
       "array([[ 2,  2,  3,  8, 26, 14, 15,  9, 11, 28,  7, 25, 13],\n",
       "       [30,  2, 18, 20, 12, 29, 10, 17, 19,  0,  0,  0,  0],\n",
       "       [22,  3,  6, 16, 21, 27,  5,  0,  0,  0,  0,  0,  0],\n",
       "       [32, 24,  4, 31, 23,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add ngrams -> vocabulary size increases.\n",
    "vectorizer = TextVectorization(ngrams=2)\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfe7b5f-a569-4470-9aa4-f437c0f380fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f59afc1f-b615-4188-a329-f3999f90ef91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'she',\n",
       " 'doesn’t',\n",
       " 'water',\n",
       " 'teach',\n",
       " 'study',\n",
       " 'paris',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'math',\n",
       " 'live',\n",
       " 'in',\n",
       " 'he',\n",
       " 'hate',\n",
       " 'german',\n",
       " 'does',\n",
       " 'cats']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 17), dtype=float32, numpy=\n",
       "array([[0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        0.],\n",
       "       [0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One-hot encode each text (sentence).\n",
    "vectorizer = TextVectorization(output_mode='multi_hot')\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c6f4e-1009-4314-b7a6-66d9b7efda01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `output_mode='count'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "166ee760-56d6-46a6-94cb-eeb27fdae0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'she',\n",
       " 'doesn’t',\n",
       " 'water',\n",
       " 'teach',\n",
       " 'study',\n",
       " 'paris',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'math',\n",
       " 'live',\n",
       " 'in',\n",
       " 'he',\n",
       " 'hate',\n",
       " 'german',\n",
       " 'does',\n",
       " 'cats']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 17), dtype=float32, numpy=\n",
       "array([[0., 2., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        0.],\n",
       "       [0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = TextVectorization(output_mode='count')\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d15c0da-6731-43f0-9d28-697a68197e12",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One-hot encoding with TF-IDF instead of ints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1c5519b2-bc14-45f7-a63a-420b234497c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'she',\n",
       " 'doesn’t',\n",
       " 'water',\n",
       " 'teach',\n",
       " 'study',\n",
       " 'paris',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'math',\n",
       " 'live',\n",
       " 'in',\n",
       " 'he',\n",
       " 'hate',\n",
       " 'german',\n",
       " 'does',\n",
       " 'cats']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 17), dtype=float32, numpy=\n",
       "array([[0.        , 1.6945957 , 0.84729785, 0.        , 0.        ,\n",
       "        1.0986123 , 0.        , 1.0986123 , 1.0986123 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.0986123 ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.84729785, 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.0986123 , 0.        , 0.        , 0.        ,\n",
       "        1.0986123 , 1.0986123 , 0.        , 0.        , 0.        ,\n",
       "        1.0986123 , 0.        ],\n",
       "       [0.        , 0.        , 0.84729785, 0.        , 1.0986123 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.0986123 ,\n",
       "        0.        , 0.        , 1.0986123 , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.0986123 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.0986123 , 0.        ,\n",
       "        0.        , 1.0986123 ]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = TextVectorization(output_mode='tf-idf')\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb4a38-ee2b-438c-bdcb-e4b47fec6900",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DEMO `tf.expand_dims()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c88341e7-8294-48b7-9e63-55f50129e1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n",
       "array([[1],\n",
       "       [2]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 2]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 2), dtype=int32, numpy=array([[[1, 2]]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1), dtype=int32, numpy=\n",
       "array([[[1]],\n",
       "\n",
       "       [[2]]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tf.expand_dims([1, 2], -1))\n",
    "display(tf.expand_dims([1, 2], 0))\n",
    "display(tf.expand_dims(tf.expand_dims([1, 2], 0), 0))\n",
    "display(tf.expand_dims(tf.expand_dims([1, 2], -1), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e7e3885-265a-4b1b-b12f-f9d8514c9901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'asd'], dtype=object)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims('asd', -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
