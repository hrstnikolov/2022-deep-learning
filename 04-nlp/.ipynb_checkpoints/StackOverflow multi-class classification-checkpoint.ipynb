{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "408c68e4-7181-4196-ba3e-cc89f3952469",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2143a2b2-3875-4941-b114-0acc140a6815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import text_dataset_from_directory\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization, Input, Dense, Dropout, Embedding, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed6452d-671d-491c-9540-9a21c98caf3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# StackOverflow Multi-class Classification\n",
    "\n",
    "The dataset contains StackOverflow posts on 4 different programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7ca77197-25f7-4713-a962-24e828b93feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee553f-b3d3-457c-b842-036f78bce841",
   "metadata": {},
   "source": [
    "## Read data and create `tf.data.Dataset`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1bbec2aa-fc85-4f72-9cce-c07b2155879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path().home() / 'Desktop\\\\datasets\\\\stackoverflow\\\\stack_overflow_16k'\n",
    "train_path = dataset_path / 'train'\n",
    "test_path = dataset_path / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5d82901f-16f6-4c64-89e6-77ad7ce913c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n",
      "Using 1600 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
       " <BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_ds, raw_val_ds = text_dataset_from_directory(\n",
    "    train_path,\n",
    "    seed=SEED,\n",
    "    validation_split=0.2,\n",
    "    subset='both'\n",
    ")\n",
    "\n",
    "raw_train_ds, raw_val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "09f4c940-1018-4f14-b86c-ecafcf27ec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test_ds = text_dataset_from_directory(\n",
    "    test_path\n",
    ")\n",
    "raw_test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d8d03-3acc-405e-8a97-b9d808a1b2b2",
   "metadata": {},
   "source": [
    "## Explore datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe71dc8-d94a-4935-8ff5-e96ae48790c7",
   "metadata": {},
   "source": [
    "### Readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7ce019c2-d777-4d6e-ad70-59197d2c7933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset is an extract from the public [Stack Overflow dataset](https://console.cloud.google.com/marketplace/details/stack-exchange/stack-overflow) for use as a tutorial on tensorflow.org. \n",
      "\n",
      "It contains the body of 16,000 posts on four languages (Java, Python, CSharp, and Javascript), which are equally divided into train and test. \n",
      "\n",
      "The keywords \"Java\", \"Python\", \"CSharp\" and \"JavaScript\" have been replaced in each post by the word \"BLANK\" in order to increase the difficulty of this dataset in classification examples.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_path / 'README.md') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4663a2-de5f-4f5c-9e29-de2eea1e60e6",
   "metadata": {},
   "source": [
    "### Sample post\n",
    "\n",
    "Показване на случаен пост. Текстът е преди почистване. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e0fde31c-50cf-46ed-8258-dda50efcdb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "================================================================================\n",
      "b'\"set blank to quit on exception? i\\'m using blank 3..i\\'ve been looking around for an answer to this, but i haven\\'t found it yet. basically, i\\'m running several blank scripts into a game engine, and each script has its own entry point...i\\'d rather not add try: except blocks through all of my code, so i was wondering if it\\'s at all possible to tell blank to quit (or perhaps assign a custom function to that \"\"callback\"\") on finding its first error, regardless of where or what it found? ..currently, the game engine will continue after finding and hitting an error, making it more difficult than necessary to diagnose issues since running into one error may make a subsequent script not work (as it relies on variables that the error-ing script set, for example). any ideas? ..i know that i could redirect the console to a file to allow for easier scrolling, but just capturing the first error and stopping the game prematurely would be really useful...okay, a couple of extra bits of info - sorry for neglecting to say this. the engine i\\'m using (the blender game engine) is coded in c, so changing the source is more than i\\'d like to do.....after googling, it would appear that a similar question with a solid answer has been asked here, which is how to get the last raised exception. if i check the sys module for the presence of the last_value variable and it exists, then i can quit prematurely, as the console would have already printed out the error...thanks for the help.\"\\n'\n"
     ]
    }
   ],
   "source": [
    "for posts_batch, labels_batch in raw_train_ds.take(1):\n",
    "    post = posts_batch[0]    \n",
    "    label = raw_train_ds.class_names[labels_batch[0]]\n",
    "    print(label, '='*80, post.numpy(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c88f9b-e7cb-4cdb-8329-3ae65cc5e468",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8a8fb-1c13-4810-bd53-9574914e47f3",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Стратегията е да подготвим dataset-овете предварително, преди да ги подадем на модела. Подготовката се състой от:\n",
    "- превръщане на текстовете във вектори (=векторизация)\n",
    "- настройване за бързо изпълнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b6a4aa76-50f2-4f68-8795-a2ea1b0b99dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.preprocessing.text_vectorization.TextVectorization at 0x26a40505270>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define vectorizer layer.\n",
    "\n",
    "MAX_TOKENS = 10_000\n",
    "OUTPUT_SEQUENCE_LENGTH = 250\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=OUTPUT_SEQUENCE_LENGTH,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "text_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3366c27c-b188-4efd-981b-5bdc68693d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teach vocabulary from training data.\n",
    "train_questions = raw_train_ds.map(lambda question, language: question)\n",
    "text_vectorizer.adapt(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "56ef7335-5d2a-4b8f-a202-4daeb69424d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'i', 'to', 'a', 'is', 'in', 'and', 'of']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out top 10 most frequent words (tokens).\n",
    "top_10 = text_vectorizer.get_vocabulary()[:10]\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "970196dc-b02b-4a5d-a183-64dcedbe0c92",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\"how to change data format in write function in blank? how to change the data format in f.write function? ..loaded_data = 349.00  or 3.00..i want to change data format in write function like %6f in print function. ..ex)  349.00 -> 349.000000 ,   3.00 -> 3.000000..f = open(\"\"test.txt\"\", \\'w\\').f.write( str.(loaded_data).zfill(?) )  ...what is the code that performs above function?\"\\n', shape=(), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[  24    4  175   80  290    7  174   38    7   16   24    4  175    2\n",
      "   80  290    7 7865   38    1    1   45    1   46    4  175   80  290\n",
      "    7  174   38   48    1    7   75   38  507    1    1 1544    1    1\n",
      "    1    1   55    6    2   30   14 3374  250   38    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(250,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Print out a sample question and its vector \n",
    "for question_batch in train_questions.take(1):\n",
    "    single_question = question_batch[0]\n",
    "    print(single_question,\n",
    "          text_vectorizer(single_question), \n",
    "          sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fcc6dc3d-ddeb-4acf-baa3-be8403100dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<MapDataset element_spec=(TensorSpec(shape=(None, 250), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
       " <MapDataset element_spec=(TensorSpec(shape=(None, 250), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
       " <MapDataset element_spec=(TensorSpec(shape=(None, 250), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vectorized datasets\n",
    "\n",
    "def vectorize_record(question, label):\n",
    "    \"\"\"Takes a single pair of question and label and \n",
    "    returns the vectorized question and unmodified label.\n",
    "    \"\"\"\n",
    "    return (text_vectorizer(tf.expand_dims(question, -1)),\n",
    "            label)\n",
    "        \n",
    "\n",
    "train_ds_vectorized = raw_train_ds.map(vectorize_record)\n",
    "val_ds_vectorized = raw_val_ds.map(vectorize_record)\n",
    "test_ds_vectorized = raw_test_ds.map(vectorize_record)\n",
    "\n",
    "train_ds_vectorized, val_ds_vectorized, test_ds_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc899428-d9b8-43de-930e-e9253e9bf659",
   "metadata": {},
   "source": [
    "### Configure for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df09b6-f7d2-4d37-8183-f87afe820450",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds_vectorized = train_ds_vectorized.cache().prefetch(AUTOTUNE)\n",
    "val_ds_vectorized   = val_ds_vectorized.cache().prefetch(AUTOTUNE)  \n",
    "test_ds_vectorized  = test_ds_vectorized.cache().prefetch(AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58fa78-fab9-4440-9bbd-ea580fedb17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9661a26d-bf2e-4028-855b-19010b478735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "186b6174-bcd2-49dd-940d-cdf2ee983171",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Demo `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6cfe865-1be3-47fa-be82-7f76af668ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a130ceda-0d79-41fd-bef6-bfcb2e72dbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.take(2).as_numpy_iterator())\n",
    "list(dataset.take(2).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "499948ff-da51-44f2-a4cf-28136831cbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for _ in dataset.take(2):\n",
    "    pass\n",
    "\n",
    "for _ in dataset.take(2):\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9147d97-6bcf-44f3-a1a0-d864e57efc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
