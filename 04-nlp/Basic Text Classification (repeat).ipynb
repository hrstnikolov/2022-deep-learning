{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5916d5af-0811-4865-a229-37de3ff37bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8d248a1c-4bb2-414c-84e9-14b30ee3ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from IPython.display import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import text_dataset_from_directory\n",
    "from tensorflow.keras.layers import TextVectorization, Input\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50555180-45ef-4710-9ca8-a28c989e8356",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basic Text Classification (repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5146bf-e6c6-4744-8f10-566389da2595",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "00922df7-5be0-40a4-8a06-09a3383e56c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SEED = 123\n",
    "BATCH_SIZE = 32\n",
    "MAX_TOKENS = 10_000  # the vocabulary would contain max of 10K words+ngrams\n",
    "OUTPUT_SEQUENCE_LENGTH = 250  # each text(record) shall be limited to 250 words(+ngrams)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "display(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a7730-cd42-42ac-9cd0-aabd2f38deab",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63a8f2-3362-43ff-909d-fe680824a472",
   "metadata": {},
   "source": [
    "### Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0e1455ae-be07-4309-8e52-2b87f43d6704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_path = os.path.join(\n",
    "    os.environ['HOME'],\n",
    "    'Desktop/datasets/aclImdb',\n",
    ")\n",
    "\n",
    "os.listdir(movies_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6039d0-c7e7-44b2-833e-56343df7d824",
   "metadata": {},
   "source": [
    "### Integrated vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ab70678b-73a1-4e23-830c-bea44065e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the, and, a, of, to, is, it, in, i, this\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(movies_path, 'imdb.vocab'), encoding='ISO-8859-1') as f:\n",
    "    vocab = [v.strip() for v in f.readlines()[:10]]\n",
    "    print(', '.join(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e237ca-bd12-4f48-b22d-9635d16396c6",
   "metadata": {},
   "source": [
    "### Sample review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "960b1d6e-ced0-4e29-909d-489f3eb1aee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any story comprises a premise, characters and conflict. Characters plotting their own play promises triumph, and a militant character readily lends oneself to this. Ardh Satya's premise is summarized by the poem of the same name scripted by Dilip Chitre. The line goes - \"ek palde mein napunsaktha, doosre palde mein paurush, aur teek tarazu ke kaante par, ardh satya ?\". A rough translation - \"The delicate balance of right & wrong ( commonly seen on the busts of blind justice in the courts ) has powerlessness on one plate and prowess on another. Is the needle on the center a half-truth ? \"<br /><br />The poem is recited midway in the film by Smita Patil to Om Puri at a resturant. It makes a deep impact on the protagonist & lays the foundation for much of the later events that follow. At the end of the film, Om Puri ends up in exactly the same situation described so aptly in the poem.<br /><br />The film tries mighty hard to do a one-up on the poem. However, Chitre's words are too powerful, and at best, the film matches up to the poem in every aspect.<br /><br />\n"
     ]
    }
   ],
   "source": [
    "path_train_positive = os.path.join(movies_path, 'train', 'pos')\n",
    "a_file = os.listdir(path_train_positive)[123]\n",
    "with open(os.path.join(path_train_positive, a_file)) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c205926-38e5-4510-abed-efe4c9630de4",
   "metadata": {},
   "source": [
    "### Directories, train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4f5c2da0-9c34-4ab0-bbd0-518de43b8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = os.path.join(movies_path, 'train')\n",
    "path_test = os.path.join(movies_path, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c02ee1-10e3-4af4-b805-40016c871fcc",
   "metadata": {},
   "source": [
    "### Datasets, training and val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ace9f74e-b773-45c3-94f3-203b681549c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Using 5000 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
       " <BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_ds, raw_val_ds = text_dataset_from_directory(\n",
    "    directory=path_train,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=None,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    validation_split=0.2,\n",
    "    subset='both',\n",
    ")\n",
    "\n",
    "raw_training_ds, raw_val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f03588-c6b9-46e2-bbb4-bca45d54215b",
   "metadata": {},
   "source": [
    "### Test: number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f46b90e2-4e63-4918-9d21-f7b9d1936c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5024)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_training = raw_training_ds.cardinality().numpy() * BATCH_SIZE\n",
    "n_validation = raw_val_ds.cardinality().numpy() * BATCH_SIZE\n",
    "\n",
    "n_training, n_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1bc36-043b-4b77-b0cb-a15e70459560",
   "metadata": {},
   "source": [
    "### Test: sample review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "553c5fa7-3706-45b2-af2b-1eb5290e7361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'After, I watched the films... I thought, \"Why the heck was this film such a high success in the Korean Box Office?\" Even thought the movie had a clever/unusal scenario, the acting wasn\\'t that good and the characters weren\\'t very interesting. For a Korean movie... I liked the fighting scenes. If you want to watch a film without thinking, this is the film for you. But I got to admit... the film was kind of childish... 6/10'\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for texts_batch, labels_batch in raw_training_ds.take(1):\n",
    "    sample_review = texts_batch[0].numpy()\n",
    "    sample_label = labels_batch[0].numpy()\n",
    "    print(sample_review, sample_label, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71127425-6a90-40de-a4fd-0942f5b7b81f",
   "metadata": {},
   "source": [
    "### Dataset, test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c994d89b-cfbe-4313-ba2f-ad9cc6177e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "display(os.path.exists(path_test))\n",
    "\n",
    "raw_test_ds = text_dataset_from_directory(\n",
    "    directory=path_test,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=None,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "69179ebd-830d-4ec2-8567-9f644a02ee97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test_ds.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27fb19-2f7c-4d85-9da0-9e042369c9fd",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Prepare the dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a7817-6dd2-43d8-9df4-d7f95100252f",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Implement a function to clean each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ccd7bba3-9cec-47cd-b93f-f9079145dba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'After, I watched the films... I thought, \"Why the heck was this film such a high success in the Korean Box Office?\" Even thought the movie had a clever/unusal scenario, the acting wasn\\'t that good and the characters weren\\'t very interesting. For a Korean movie... I liked the fighting scenes. If you want to watch a film without thinking, this is the film for you. But I got to admit... the film was kind of childish... 6/10'\n",
      "\n",
      "tf.Tensor(b'after i watched the films i thought why the heck was this film such a high success in the korean box office even thought the movie had a cleverunusal scenario the acting wasnt that good and the characters werent very interesting for a korean movie i liked the fighting scenes if you want to watch a film without thinking this is the film for you but i got to admit the film was kind of childish 610', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "def clean_review(review):\n",
    "    cleaned = tf.strings.lower(review)\n",
    "    cleaned = tf.strings.regex_replace(cleaned, '<[^>]+>', '')\n",
    "    cleaned = tf.strings.regex_replace(cleaned, f'[{re.escape(string.punctuation)}]', '')\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "print(sample_review, clean_review(sample_review), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717428bb-0390-41aa-97b0-9e573cd3552f",
   "metadata": {},
   "source": [
    "### `TextVectorization` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e39fcedc-7b92-4274-a2df-9e0297ec1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    standardize=clean_review,\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=OUTPUT_SEQUENCE_LENGTH,\n",
    "    encoding='utf-8',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a6de9-51f7-461f-989c-a7e59df2eda8",
   "metadata": {},
   "source": [
    "### Datasets, only input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a7b06218-cb40-4585-9e80-29bcfeca557c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "acd53f9e-c113-45a8-ba8a-d7b4758cc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = raw_training_ds.map(lambda review, label: review)\n",
    "# train_labels = raw_training_ds.map(lambda review, label: review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0310c825-e8dc-46d7-97dc-94fcca00b46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d5bd4b22-792e-4cf7-9037-c33dc1b72b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8992b927-210d-438a-9271-97b22b05eda5",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "496f5bfa-7112-4f8e-bf53-97b206f22be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute vocab.\n",
    "# Vocab is ordered (most frequent words first).\n",
    "vectorize_layer.adapt(training_ds)\n",
    "vectorize_layer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da8743b-825a-4d32-b909-2f2361f0916f",
   "metadata": {},
   "source": [
    "### Test: sample review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3b9b489e-1344-4911-9f3f-72789fdc2f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"This Documentary (Now available free on Video.Google.Com) is a fantastic demonstration of the power of ordinary people to overcome injustice. Everyone must see this.<br /><br />Chavez was elected in a landslide vote in 1998. His platform was to divert the fantastic oil wealth from the 20% middle class to the 80% poor. He banned foreign drift net fishing in Venezuelan waters. He sent 10,000 Cuban doctors to the slums to treat the sick for free. He wiped out illiteracy and set up new free Universities. <br /><br />But it was his 30% tax on oil company profits that got him in trouble with the Bush administration. In 2002, while Irish film makers Kim Bartley and Donnacha O'Briain were interviewing Chavez inside the Presidential Palace about his social programs, a CIA backed coup was launched. With the cameras rolling, Chavez was captured and flown out of the country. It was announced on national TV that he had 'resigned'.<br /><br />But the poor of Venezuela didn't believe the media. They went to the Palace in their millions and demanded that Chavez be returned. In the face of such overwhelming numbers, the military turned on the coup leaders and the plotters fled to the US. Chavez was rescued by military helicopter and returned to jubilation.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(250,), dtype=int64, numpy=\n",
       "array([  10,  676,  158, 1412,  962,   20,    1,    7,    4,  768,    1,\n",
       "          5,    2,  664,    5, 1809,   77,    6, 3042, 8554,  302,  217,\n",
       "         65,    1,   13, 8402,    8,    4,    1, 2187,    8, 6825,   24,\n",
       "       7397,   13,    6,    1,    2,  768, 3340, 3671,   36,    2,  955,\n",
       "        759,  797,    6,    2, 3461,  339,   27, 3921, 2060, 8011, 5767,\n",
       "       5627,    8,    1, 3956,   27, 1345,    1, 6336, 3835,    6,    2,\n",
       "          1,    6, 1656,    2, 1187,   16,  962,   27, 6585,   44,    1,\n",
       "          3,  272,   54,  153,  962,    1,   18,    9,   13,   24, 1183,\n",
       "       6840,   20, 3340, 1145, 9978,   12,  183,   88,    8, 1137,   15,\n",
       "          2, 3458, 9612,    8, 3847,  136, 2524,   19, 1346, 2560,    1,\n",
       "          3,    1,    1,   66,    1, 6344,  979,    2, 8951, 5388,   42,\n",
       "         24, 1007, 5528,    4, 3600, 7342, 5801,   13,    1,   15,    2,\n",
       "       3948, 2698, 6344,   13, 1798,    3,    1,   44,    5,    2,  694,\n",
       "          9,   13, 8070,   20, 1987,  245,   12,   27,   67,    1,    2,\n",
       "        339,    5,    1,  150,  253,    2, 1802,   34,  412,    6,    2,\n",
       "       5388,    8,   64, 3022,    3,    1,   12, 6344,   26, 3616,    8,\n",
       "          2,  376,    5,  135, 4269, 1494,    2, 1239,  660,   20,    2,\n",
       "       5801, 5611,    3,    2,    1,    1,    6,    2,  169, 6344,   13,\n",
       "       6616,   32, 1239, 4055,    3, 3616,    6,    1,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int64)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The review printed below is before preprocessing \n",
    "# that is before lowercasing, removing punctuation and html.\n",
    "sample_review = list(training_ds.as_numpy_iterator())[0][0]\n",
    "display(sample_review)\n",
    "display(vectorize_layer(sample_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d836c83-af4d-4dd4-928d-148350fd8156",
   "metadata": {},
   "source": [
    "### Final datasets\n",
    "\n",
    "Contain both input (vectorized amazon review) and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e3d854a3-7126-44a4-a03f-8357c9f249ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(None, 250), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(None, 250), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(None, 250), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def vectorize_ds(review, label):\n",
    "    review = vectorize_layer(review)\n",
    "    return review, label\n",
    "\n",
    "train_ds = raw_training_ds.map(vectorize_ds)\n",
    "val_ds = raw_val_ds.map(vectorize_ds)\n",
    "test_ds = raw_test_ds.map(vectorize_ds)\n",
    "\n",
    "display(train_ds)\n",
    "display(val_ds)\n",
    "display(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939aff6e-005e-4863-9262-379e2e647167",
   "metadata": {},
   "source": [
    "### Test: train batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a68e08c7-134b-4588-a7f4-0136af04f374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 250), dtype=int64, numpy=\n",
       " array([[1872,  704,    6, ...,    0,    0,    0],\n",
       "        [  11,   25,  200, ...,    0,    0,    0],\n",
       "        [   5,   31,    2, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [  48,   23,  105, ...,  184,   12,   24],\n",
       "        [   1,   10, 4302, ...,    0,    0,    0],\n",
       "        [  10,   17,    7, ...,    0,    0,    0]], dtype=int64)>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 0, 0, 0, 0])>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_train_batch = next(iter(train_ds))\n",
    "first_train_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9635cc22-481d-4c15-83d7-fc202ed000ed",
   "metadata": {},
   "source": [
    "## Configure for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "43369576-fc9b-4b2c-bbd7-d5776363496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e74f3e3-be1b-4905-b655-a3269df463fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfa7fc-a52b-46d5-9d9e-1fb5a1f0e4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da87ca-f211-44c6-8561-97ab35f04b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20654fe-5f9e-4b28-be72-62ccf9b05a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76279a87-93e6-4c04-9a99-ae6dfd7e594e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe46b4-d1bc-40ef-bcc7-5e98c88821ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f5b1d-3605-4339-8ffb-708f74c7b012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73898198-e3f6-4a8f-9c9f-cf879152b0de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## DEMO `TextVectorization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c129cc0-4a99-4986-956a-8a4b92412f7e",
   "metadata": {},
   "source": [
    "### Define a Vectorizer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9cd70d14-1b08-4094-b196-8697517ba84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(name='dummy_vectorizer')\n",
    "texts = [\n",
    "\t\"She she doesn’t study German on Monday.\",\n",
    "\t\"Does she live in Paris?\",\n",
    "\t\"He doesn’t teach math.\",\n",
    "\t\"Cats hate water.\",\n",
    "\t# \"Every child likes an ice cream.\",\n",
    "\t# \"My brother takes out the trash.\",\n",
    "\t# \"The course starts next Sunday.\",\n",
    "\t# \"She swims every morning.\",\n",
    "\t# \"I don’t wash the dishes.\",\n",
    "\t# \"We see them every week.\",\n",
    "\t# \"I don’t like tea.\",\n",
    "\t# \"When does the train usually leave?\",\n",
    "\t# \"She always forgets her purse.\",\n",
    "\t# \"You don’t have children.\",\n",
    "\t# \"I and my sister don’t see each other anymore.\",\n",
    "\t# \"They don’t go to school tomorrow.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580f5fc-6a04-4b7d-bf80-37e0daf25da1",
   "metadata": {},
   "source": [
    "### Teach vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ff1cfbc7-6a62-43a9-9dd0-397676a4aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15918a7-9a82-4bab-a00f-b4597446ed16",
   "metadata": {},
   "source": [
    "### Display some attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "382a1849-9e0a-4f2c-bde5-49ae2fa93172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'string'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dummy_vectorizer'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'she', 'doesn’t', 'water', 'teach', 'study', 'paris']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# `[UNK]` =  unknown word\n",
    "display(vectorizer.vocabulary_size())\n",
    "display(vectorizer.dtype)\n",
    "display(vectorizer.name)\n",
    "display(vectorizer.get_vocabulary()[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e921c0a-ced4-4e34-bb23-4e43e5c36ce8",
   "metadata": {},
   "source": [
    "### Default integer encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "74a8f483-4cc6-4740-9167-21102c0bdafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 7), dtype=int64, numpy=\n",
       "array([[ 2,  2,  3,  6, 15,  8,  9],\n",
       "       [16,  2, 11, 12,  7,  0,  0],\n",
       "       [13,  3,  5, 10,  0,  0,  0],\n",
       "       [17, 14,  4,  0,  0,  0,  0]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Default integer encoding: integer indices, one integer index per split string token.\n",
    "# Each sentence need to be a separate record -> `tf.expand_dims()`.\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f2742-0c25-42e1-af43-e7fe13c77364",
   "metadata": {},
   "source": [
    "### Cap the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aab59724-bac3-4090-afd4-3f440603340a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'she', 'doesn’t', 'water', 'teach', 'study', 'paris']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 7), dtype=int64, numpy=\n",
       "array([[2, 2, 3, 6, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 7, 0, 0],\n",
       "       [1, 3, 5, 1, 0, 0, 0],\n",
       "       [1, 1, 4, 0, 0, 0, 0]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cap the number of tokens -> a lot of `[UNK]` (unknown words).\n",
    "vectorizer = TextVectorization(max_tokens=8)\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9024a90-7041-470e-939d-b64fcb9e7adf",
   "metadata": {},
   "source": [
    "### Increase the len of the output encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "308c9ec0-aacf-406c-8c74-1a682cbf0957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'she', 'doesn’t', 'water', 'teach', 'study', 'paris']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 20), dtype=int64, numpy=\n",
       "array([[2, 2, 3, 6, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 2, 1, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Increase the len of the output encoding -> trailing zeros.\n",
    "vectorizer = TextVectorization(max_tokens=8, output_sequence_length=20)\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e4548-02db-442b-867d-606905c0d535",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b5f578d1-1198-487d-b520-152da6d9b269",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'she',\n",
       " 'doesn’t',\n",
       " 'water',\n",
       " 'teach math',\n",
       " 'teach',\n",
       " 'study german',\n",
       " 'study',\n",
       " 'she she',\n",
       " 'she live',\n",
       " 'she doesn’t',\n",
       " 'paris',\n",
       " 'on monday',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'math',\n",
       " 'live in',\n",
       " 'live',\n",
       " 'in paris',\n",
       " 'in',\n",
       " 'he doesn’t',\n",
       " 'he',\n",
       " 'hate water',\n",
       " 'hate',\n",
       " 'german on',\n",
       " 'german',\n",
       " 'doesn’t teach',\n",
       " 'doesn’t study',\n",
       " 'does she',\n",
       " 'does',\n",
       " 'cats hate',\n",
       " 'cats']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 13), dtype=int64, numpy=\n",
       "array([[ 2,  2,  3,  8, 26, 14, 15,  9, 11, 28,  7, 25, 13],\n",
       "       [30,  2, 18, 20, 12, 29, 10, 17, 19,  0,  0,  0,  0],\n",
       "       [22,  3,  6, 16, 21, 27,  5,  0,  0,  0,  0,  0,  0],\n",
       "       [32, 24,  4, 31, 23,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add ngrams -> vocabulary size increases.\n",
    "vectorizer = TextVectorization(ngrams=2)\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfe7b5f-a569-4470-9aa4-f437c0f380fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f59afc1f-b615-4188-a329-f3999f90ef91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'she',\n",
       " 'doesn’t',\n",
       " 'water',\n",
       " 'teach',\n",
       " 'study',\n",
       " 'paris',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'math',\n",
       " 'live',\n",
       " 'in',\n",
       " 'he',\n",
       " 'hate',\n",
       " 'german',\n",
       " 'does',\n",
       " 'cats']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 17), dtype=float32, numpy=\n",
       "array([[0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        0.],\n",
       "       [0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One-hot encode each text (sentence).\n",
    "vectorizer = TextVectorization(output_mode='multi_hot')\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c6f4e-1009-4314-b7a6-66d9b7efda01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `output_mode='count'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "166ee760-56d6-46a6-94cb-eeb27fdae0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'she',\n",
       " 'doesn’t',\n",
       " 'water',\n",
       " 'teach',\n",
       " 'study',\n",
       " 'paris',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'math',\n",
       " 'live',\n",
       " 'in',\n",
       " 'he',\n",
       " 'hate',\n",
       " 'german',\n",
       " 'does',\n",
       " 'cats']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 17), dtype=float32, numpy=\n",
       "array([[0., 2., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        0.],\n",
       "       [0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = TextVectorization(output_mode='count')\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d15c0da-6731-43f0-9d28-697a68197e12",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One-hot encoding with TF-IDF instead of ints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1c5519b2-bc14-45f7-a63a-420b234497c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'she',\n",
       " 'doesn’t',\n",
       " 'water',\n",
       " 'teach',\n",
       " 'study',\n",
       " 'paris',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'math',\n",
       " 'live',\n",
       " 'in',\n",
       " 'he',\n",
       " 'hate',\n",
       " 'german',\n",
       " 'does',\n",
       " 'cats']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['She she doesn’t study German on Monday.',\n",
       " 'Does she live in Paris?',\n",
       " 'He doesn’t teach math.',\n",
       " 'Cats hate water.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 17), dtype=float32, numpy=\n",
       "array([[0.        , 1.6945957 , 0.84729785, 0.        , 0.        ,\n",
       "        1.0986123 , 0.        , 1.0986123 , 1.0986123 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.0986123 ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.84729785, 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.0986123 , 0.        , 0.        , 0.        ,\n",
       "        1.0986123 , 1.0986123 , 0.        , 0.        , 0.        ,\n",
       "        1.0986123 , 0.        ],\n",
       "       [0.        , 0.        , 0.84729785, 0.        , 1.0986123 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.0986123 ,\n",
       "        0.        , 0.        , 1.0986123 , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.0986123 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.0986123 , 0.        ,\n",
       "        0.        , 1.0986123 ]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = TextVectorization(output_mode='tf-idf')\n",
    "vectorizer.adapt(texts)\n",
    "display(vectorizer.get_vocabulary())\n",
    "\n",
    "display(texts)\n",
    "display(vectorizer(tf.expand_dims(texts, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb4a38-ee2b-438c-bdcb-e4b47fec6900",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DEMO `tf.expand_dims()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c88341e7-8294-48b7-9e63-55f50129e1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n",
       "array([[1],\n",
       "       [2]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 2]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 2), dtype=int32, numpy=array([[[1, 2]]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1), dtype=int32, numpy=\n",
       "array([[[1]],\n",
       "\n",
       "       [[2]]])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tf.expand_dims([1, 2], -1))\n",
    "display(tf.expand_dims([1, 2], 0))\n",
    "display(tf.expand_dims(tf.expand_dims([1, 2], 0), 0))\n",
    "display(tf.expand_dims(tf.expand_dims([1, 2], -1), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e7e3885-265a-4b1b-b12f-f9d8514c9901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'asd'], dtype=object)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims('asd', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84931368-8163-457e-80b9-7471d98d373e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DEMO Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c97e4588-9b15-4a49-a8e4-ca26b20163f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Failed to create a NewWriteableFile: /tmp_cache_0.lockfile : Access is denied.\r\n; Input/output error [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15984\\4126751168.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Iterating happens here!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4768\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4770\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4772\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    785\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[1;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 770\u001b[1;33m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    771\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3041\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3043\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3044\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3045\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7213\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7214\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7215\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Failed to create a NewWriteableFile: /tmp_cache_0.lockfile : Access is denied.\r\n; Input/output error [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# Make folder to store cache files.\n",
    "cache_dir = '/tmp_cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.mkdir(cache_dir)\n",
    "\n",
    "    \n",
    "train_ds = train_ds.cache(cache_dir)\n",
    "iterator = train_ds.as_numpy_iterator()\n",
    "\n",
    "# Iterating happens here!\n",
    "list(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb1a6c-1e98-4ba1-906a-08d512797ed2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DEMO `tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "33ef0a80-cbbb-465e-abbc-d8ce218267bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.INFINITE_CARDINALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9cbfac31-595a-42db-a67b-d248a2fb32d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.UNKNOWN_CARDINALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8b9ddb24-37e7-4977-af75-7530c400370f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.DatasetV2"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "7d057054-3822-4a99-b9e1-0ff1621a4011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024213f5-6e9e-47c4-9e74-eaf8435b338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.FixedLengthRecordDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8949300-f29c-4ed7-9bdf-228f45bc1aa6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DEMO Binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "373d8f4f-2891-4236-8d0b-e91a4250941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bytes stored in the file: \tb'\\xd1\\x8e'\n",
      "Unicode value for ю: \t\t1102\n",
      "0x44e\n"
     ]
    }
   ],
   "source": [
    "filename = 'dummy_text.txt'\n",
    "some_text = 'ю'\n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(some_text)\n",
    "    \n",
    "with open(filename, 'br') as f:\n",
    "    print(f'Bytes stored in the file: \\t{f.read()}')\n",
    "    \n",
    "print(f'Unicode value for {some_text}: \\t\\t{ord(some_text)}')\n",
    "print(hex(ord(some_text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
