{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cc2833-00a2-4ef8-8196-988dfa5a7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737df90e-20d5-4f91-b553-81ec66aba1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import text_dataset_from_directory\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, \\\n",
    "    TextVectorization, Embedding, Dropout, GlobalAveragePooling1D, Activation\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31b3e1-ded7-4ae3-82dd-c32c67649cfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basic Text Classification - part 2\n",
    "Follow-up from part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e453ca1-1df3-4bea-82fa-9467bc325ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3be3f3-858c-404f-ab53-50f8b4beef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = Path().home() / r'Desktop\\datasets\\aclImdb'\n",
    "train_dir = dataset_dir / 'train'\n",
    "test_dir = dataset_dir / 'test'\n",
    "\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "raw_train_ds = text_dataset_from_directory(\n",
    "    directory=train_dir.as_posix(), \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3235d6fa-c5b7-4572-94df-292388d7cfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "raw_val_ds = text_dataset_from_directory(\n",
    "    directory=train_dir.as_posix(), \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d05a01e9-e5b3-4f95-a7a5-82eeddf77d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_test_ds = text_dataset_from_directory(\n",
    "    directory=test_dir.as_posix(), \n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336af046-3061-4fa0-8a16-5a8f225befa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21f37316-4ac2-4a11-b95c-c7b96a902111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8d736b7-cdd6-4aa4-a379-7f017ae67607",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "607aae3c-56e2-4f76-8515-4c1cf027eb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\a1056968\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# Make a text-only dataset (without labels)\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "\n",
    "# Fit the vectorize obj with the tesxts\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adb5b8c4-3703-4b36-bdf8-1f2ff0410216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add a dimension to vectorize\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f833fc72-2d1c-4a79-9b9d-3e90fe28507f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'Great movie - especially the music - Etta James - \"At Last\". This speaks volumes when you have finally found that special someone.', shape=(), dtype=string)\n",
      "Label neg\n",
      "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
      "array([[  86,   17,  260,    2,  222,    1,  571,   31,  229,   11, 2418,\n",
      "           1,   51,   22,   25,  404,  251,   12,  306,  282,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "\n",
    "# take the first observation\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66e2feed-664e-425f-bf1f-0a9bbf86fd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287 --->  silent\n",
      " 313 --->  night\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
    "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
    "print(f'Vocabulary size: {len(vectorize_layer.get_vocabulary())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c13d8b7-1e6f-4602-bffb-ad5566312df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final datasets by\n",
    "# mapping the TextVectorization layer\n",
    "\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f9c58-4df9-4c47-8dc4-7210b1833ccd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60a86980-700c-46af-9750-e41638f16944",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e6f35-303c-43a7-8937-6ddab449e949",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the model\n",
    "\n",
    "- > The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings).\n",
    "- Below, we embed a 10,001-word vocabulary (`max_features` + 1) into 16 dimensions.\n",
    "- The weight are initially random and are gradually adjusted via backpropagation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfd9c214-5f40-4d6d-b3b9-1b7f124c6478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Embedding(max_features + 1, embedding_dim),\n",
    "    Dropout(0.2),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db263e32-7b4c-46cb-9b79-1bbbc0346b86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loss function and optimizer\n",
    "\n",
    "- single column contains the output var -> `from_logits=True`\n",
    "- Classes are ballanced -> accuracy is good performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "587c0009-a922-4933-b0a8-86f8c00afb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80890342-9267-4d68-9a5a-97a81dd6e77a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad362771-a150-4702-a58f-ccc6fa4ab4f1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 15s 21ms/step - loss: 0.6621 - binary_accuracy: 0.6999 - val_loss: 0.6121 - val_binary_accuracy: 0.7744\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.5458 - binary_accuracy: 0.8029 - val_loss: 0.4959 - val_binary_accuracy: 0.8252\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.4427 - binary_accuracy: 0.8461 - val_loss: 0.4183 - val_binary_accuracy: 0.8480\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3763 - binary_accuracy: 0.8665 - val_loss: 0.3724 - val_binary_accuracy: 0.8628\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3338 - binary_accuracy: 0.8792 - val_loss: 0.3441 - val_binary_accuracy: 0.8680\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.3036 - binary_accuracy: 0.8903 - val_loss: 0.3253 - val_binary_accuracy: 0.8708\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2802 - binary_accuracy: 0.8985 - val_loss: 0.3120 - val_binary_accuracy: 0.8730\n",
      "Epoch 8/10\n",
      "562/625 [=========================>....] - ETA: 0s - loss: 0.2594 - binary_accuracy: 0.9054"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce499a-2a2c-4dec-98f6-e3eca5bd3115",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d0d45-04e4-4c86-b654-d1b99d3a754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9f9fc-3e45-411f-8f7e-b0d44bbc611e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a plot of accuracy and loss over time\n",
    "\n",
    "Training vs validation accuracy:\n",
    "- The training accuracy peaks earlier.\n",
    "- After several epochs training gets larger than validation meaning the model over-fits the data. Solution: stop when valid acc no longer increases significantly (early stopping). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6596a1f-4b29-4713-8241-17273534c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b82ddb-8b08-422e-9696-4e9744521f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b-o', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r-o', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac9d59-bb6d-49e5-97ba-577b710f4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'b-o', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r-o', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34422eba-bb93-4bef-a246-56ff1f60c959",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Export the model\n",
    "\n",
    "- Add the preprocess layer to allow to directly predict raw IMDS reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770386a-0dd1-43b6-8ca2-db5b8d11be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3367b281-345e-4eff-a271-4a39c7f68591",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fbc054-5467-4391-a48d-8d38a638e304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed17dfb-7db0-4314-824e-fc96c4c93867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
